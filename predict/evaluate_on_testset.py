import torch
import torch.nn as nn
from torchvision import models, transforms, datasets
from PIL import Image
import numpy as np
from collections import defaultdict

# --- å‡†å¤‡å·¥ä½œï¼šéœ€è¦å’Œä½ è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ ---

# 1. æŠŠä½ çš„æ¨¡åž‹å®šä¹‰å¤åˆ¶è¿‡æ¥
class RotationPredictionModel(nn.Module):
    def __init__(self, num_classes=4):
        super(RotationPredictionModel, self).__init__()
        self.encoder = models.resnet18(weights=None)
        num_features = self.encoder.fc.in_features
        self.encoder.fc = nn.Identity()
        self.classifier = nn.Linear(num_features, num_classes)
    def forward(self, x):
        features = self.encoder(x)
        outputs = self.classifier(features)
        return outputs

# 2. å®šä¹‰å’Œä½ éªŒè¯æ—¶ä¸€æ ·çš„å›¾åƒé¢„å¤„ç†æ­¥éª¤
inference_transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# 3. åŠ è½½æ¨¡åž‹
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CHECKPOINT_PATH = "model/rotation_checkpoint.pth" # å­˜æ¡£æ–‡ä»¶è·¯å¾„
model = RotationPredictionModel(num_classes=4).to(DEVICE)

# å…ˆåŠ è½½æ•´ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¸€ä¸ªå­—å…¸ï¼‰
print(f"æ­£åœ¨ä»Ž '{CHECKPOINT_PATH}' åŠ è½½æ£€æŸ¥ç‚¹...")
checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
# ç„¶åŽä»Žå­—å…¸ä¸­åªæå–æ¨¡åž‹çš„æƒé‡çŠ¶æ€
model.load_state_dict(checkpoint['model_state_dict'])
# -------------------------

model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼
print("æ¨¡åž‹åŠ è½½æˆåŠŸï¼Œå·²è¿›å…¥è¯„ä¼°æ¨¡å¼ã€‚")

# --- ä¿®æ”¹ï¼šä»Žé¢„æµ‹å•å¼ å›¾ç‰‡æ”¹ä¸ºåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ---

# 4. åŠ è½½CIFAR-10æµ‹è¯•é›†
# æ³¨æ„ï¼šè¿™é‡Œtransform=Noneï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŽŸå§‹çš„PIL Imageæ¥è¿›è¡Œæ‰‹åŠ¨æ—‹è½¬
test_dataset_raw = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)
cifar10_labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
NUM_IMAGES_TO_TEST = 50 # æƒ³æµ‹è¯•çš„å›¾ç‰‡æ•°é‡

print(f"\n--- å°†åœ¨CIFAR-10æµ‹è¯•é›†çš„å‰ {NUM_IMAGES_TO_TEST} å¼ å›¾ç‰‡ä¸Šè¿›è¡Œè¯„ä¼° ---")

# åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
total_predictions = 0
correct_predictions = 0
angle_stats = defaultdict(lambda: {'correct': 0, 'total': 0})

# 5. å¾ªçŽ¯éåŽ†æŒ‡å®šæ•°é‡çš„æµ‹è¯•å›¾ç‰‡
for i in range(NUM_IMAGES_TO_TEST):
    # ä»Žæ•°æ®é›†ä¸­èŽ·å–ä¸€å¼ åŽŸå§‹å›¾ç‰‡å’Œå®ƒçš„çœŸå®žç±»åˆ«æ ‡ç­¾
    original_img, original_label_idx = test_dataset_raw[i]
    original_label_name = cifar10_labels[original_label_idx]

    print(f"\n===== æµ‹è¯•å›¾ç‰‡ {i+1}/{NUM_IMAGES_TO_TEST} | åŽŸå§‹ç±»åˆ«: {original_label_name} =====")

    # å¯¹è¿™å¼ å›¾ç‰‡è¿›è¡Œå››ç§è§’åº¦çš„æ—‹è½¬æµ‹è¯•
    for angle in [0, 90, 180, 270]:
        # æ‰‹åŠ¨æ—‹è½¬å›¾ç‰‡
        rotated_img = original_img.rotate(angle)
        
        # é¢„å¤„ç†å¹¶å¢žåŠ ä¸€ä¸ªbatchç»´åº¦
        img_tensor = inference_transform(rotated_img).unsqueeze(0).to(DEVICE)

        # ä¸è®¡ç®—æ¢¯åº¦ï¼Œè¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            outputs = model(img_tensor)
            _, predicted_idx = torch.max(outputs, 1)

        # å°†é¢„æµ‹ç»“æžœè§£ç å›žè§’åº¦
        angle_map = {0: 0, 1: 90, 2: 180, 3: 270}
        predicted_angle = angle_map[predicted_idx.item()]
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        total_predictions += 1
        angle_stats[angle]['total'] += 1
        if predicted_angle == angle:
            correct_predictions += 1
            angle_stats[angle]['correct'] += 1
            print(f"  - çœŸå®žæ—‹è½¬: {angle:>3}Â° -> æ¨¡åž‹é¢„æµ‹: {predicted_angle:>3}Â°  (æ­£ç¡® ðŸŽ‰)")
        else:
            print(f"  - çœŸå®žæ—‹è½¬: {angle:>3}Â° -> æ¨¡åž‹é¢„æµ‹: {predicted_angle:>3}Â°  (é”™è¯¯ âŒ)")

# æ‰“å°ç»Ÿè®¡ç»“æžœ
print("\n=== è¯„ä¼°ç»“æžœç»Ÿè®¡ ===")
print(f"æ€»ä½“æ­£ç¡®çŽ‡: {100 * correct_predictions / total_predictions:.2f}% ({correct_predictions}/{total_predictions})")
print("\nå„è§’åº¦æ­£ç¡®çŽ‡:")
for angle in sorted(angle_stats.keys()):
    stats = angle_stats[angle]
    accuracy = 100 * stats['correct'] / stats['total']
    print(f"{angle:>3}Â°: {accuracy:.2f}% ({stats['correct']}/{stats['total']})")
